{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b340317",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# P05 : Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffb102",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "In this course we learn to read data from different sources and transform the data using Pandas dataframe techniques and more including:\n",
    "\n",
    "    Transforming data using apply and map functions\n",
    "    Transforming data using dfply verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ddc63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cheat sheets\n",
    "\n",
    "\n",
    "Pandas Cheat Sheet\n",
    "\n",
    "https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cfc85c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data wrangling\n",
    "\n",
    "from wikipedia:\n",
    "\n",
    "Data munging or data wrangling is loosely the process of manually converting or mapping data from one “raw” form into another format that allows for more convenient consumption of the data with the help of semi-automated tools.\n",
    "\n",
    "In almost every dataset, it is necessary to perform some data munging before you can use the data to perform eg. a classification task. Therefore, as a data scientist, you’ll spend a lot of time munging and wrangling the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8da3df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data manipulation in Python\n",
    "\n",
    "Let us first look at munging techniques available in basic Python:\n",
    "\n",
    "    Renaming columns\n",
    "    Adding or removing columns\n",
    "    Reordering columns\n",
    "    Merging data frames\n",
    "    Finding and removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a671aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Load Iris dataset as dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656130b7",
   "metadata": {},
   "source": [
    "The iris dataset is included in the sklearn.datasets. However, the dataset was changed in scikit-learn version 0.20 and fixed two wrong data points according to Fisher’s paper. The new version is the same as in R, but not as in the UCI Machine Learning Repository.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb563bf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# let's import a few libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b51250",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# from packaging import version\n",
    "# import sklearn\n",
    "\n",
    "#print(sklearn.__version__)\n",
    "#if version.parse(sklearn.__version__) < version.parse(\"0.23.0\"):\n",
    "    # https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset\n",
    "    # save load_iris() sklearn dataset to iris\n",
    "    # if you'd like to check dataset type use: type(load_iris())\n",
    "    # if you'd like to view list of attributes use: dir(load_iris())\n",
    "#    iris_bunch = load_iris()\n",
    "\n",
    "    # np.c_ is the numpy concatenate function\n",
    "    # which is used to concat iris['data'] and iris['target'] arrays \n",
    "    # for pandas column argument: concat iris['feature_names'] list\n",
    "    # and string list (in this case one string); you can make this anything you'd like..  \n",
    "    # the original dataset would probably call this ['Species']\n",
    "#    df_iris = pd.DataFrame(data= np.c_[iris_bunch['data'], iris_bunch['target']],\n",
    "#                         columns= iris_bunch['feature_names'] + ['target'])\n",
    "#else:\n",
    "    # New in scikit-learn version 0.23: load directly into dataframe\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "#    print(\"Load data as dataframe\")\n",
    "#    iris_tuple = load_iris(as_frame=True, return_X_y=True)\n",
    "#    df_iris = iris_tuple[0]\n",
    "#    df_iris[\"target\"] = iris_tuple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40708d55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/38105539/how-to-convert-a-scikit-learn-dataset-to-a-pandas-dataset\n",
    "# if you'd like to check dataset type use: type(load_iris())\n",
    "# if you'd like to view list of attributes use: dir(load_iris())\n",
    "\n",
    "# save load_iris() sklearn dataset to iris_bunch\n",
    "iris_bunch = load_iris()\n",
    "\n",
    "# list of attributes\n",
    "dir(iris_bunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7629388",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# so what's in iris_bunch?\n",
    "\n",
    "# sample of the data\n",
    "iris_bunch.data[1:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ad1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with feature names \n",
    "iris_bunch.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d29bae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# target variable (3 different species)\n",
    "iris_bunch.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be556e46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# np.c_ is the numpy concatenate function\n",
    "# which is used to concat iris['data'] and iris['target'] arrays \n",
    "# for pandas column argument: concat iris['feature_names'] list\n",
    "# and string list (in this case one string); you can make this anything you'd like..  \n",
    "# the original dataset would probably call this ['Species']\n",
    "df_iris = pd.DataFrame(data= np.c_[iris_bunch['data'], iris_bunch['target']],\n",
    "                         columns= iris_bunch['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142cc49",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There are even more options to get the iris dataset. For instance, the seaborn package to create plots has a function to load the iris dataset from the internet The code even more straith forward and the data is directly available in a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e4679a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_iris_seaborn = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba06b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Renaming columns in a data frame\n",
    "\n",
    "You can do any of the following with Python’s built-in functions. Note that these modify iris dataframe directly; that is, you don’t have to save the result back into df_iris when using the \"inplace\" argument with a value set to True. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a30383",
   "metadata": {},
   "source": [
    "#### Rename the column named \"sepal length (cm)\" to \"sepal lengte (cm)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fc35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.rename(columns={'sepal length (cm)':'sepal lengte (cm)'}, inplace=True)\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb451a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Rename by index in names list: change third item, \"petal length (cm)\", to \"petal lengte (cm)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea31f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.rename(columns={ df_iris.columns[2]: \"petal lengte (cm)\" }, inplace = True)\n",
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ecd6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Show all column names as a list, including the modified column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a9c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iris.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb8ebb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adding and removing columns in a data frame\n",
    "\n",
    "There are many different ways of adding columns to a dataframe and removing columns from a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e899847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method used often to create a dataframe quickly is to define a dictionary containing your column names and data:\n",
    "data = {'id': [1,2,3],\n",
    "        'weight': [20,27,24]}\n",
    "  \n",
    "# ... and convert the dictionary into dataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018f31b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Different ways to add a column to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By declaring a new list as a column\n",
    "# Declare a list that is to be converted into a column\n",
    "size = [\"small\", \"large\", \"medium\"]\n",
    "  \n",
    "# Using 'size' as the column name\n",
    "# and equating it to the list\n",
    "df['size1'] = size\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668cc761",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Using DataFrame.insert() to add a column\n",
    "# The first argument is the position to insert the new column at\n",
    "df.insert(len(df), \"size2\", [\"small\", \"large\", \"medium\"], True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b2107",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Using Dataframe.assign() method\n",
    "# This will create a new dataframe with the new column added to the old dataframe.\n",
    "\n",
    "# Using 'size3' as the column name and equating it to the list\n",
    "df = df.assign(size3 = [\"small\", \"large\", \"medium\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abadcf71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Using a Python dictionary to add a column\n",
    "\n",
    "# Define a dictionary with key values of an existing column and their respective\n",
    "# value pairs as the values for our new column.\n",
    "\n",
    "size4 = {1: 'small', 2: 'large', 3: 'medium'}\n",
    "\n",
    "df['size4'] = df['id'].map(size4)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51768a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Different ways to remove a column from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using del\n",
    "del df['size1']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ede83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the dataframe drop method\n",
    "df.drop('size2', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4913de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# by listing the columns you would like to keep\n",
    "# and dropping any columns not listed\n",
    "df=df[['id','size4']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8038ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pop method is used to remove the specified column from the DataFrame and return the removed column as a pandas Series.\n",
    "removed_col=df.pop('size4')\n",
    "removed_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41698a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Lets see what is left of the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba443f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Reordering columns in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d38489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataframe as we did earlier\n",
    "# Define a dictionary containing data\n",
    "data = {\n",
    "        'id': [1,2,3],\n",
    "        'weight': [20,27,24],\n",
    "        'size': [\"small\", \"large\", \"medium\"]\n",
    "}\n",
    "# Convert the dictionary into DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7483738d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Reorder by column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using double brackets to reorder columns\n",
    "df = df[['weight', 'id', 'size']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb8f10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Using pandas.DataFrame.reindex() to reorder columns in a DataFrame\n",
    "column_names = ['size', 'weight', 'id']\n",
    "df = df.reindex(columns=column_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa4d955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Reorder columns to the original sequence\n",
    "df = df[['id', 'weight', 'size']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get values in a row using the row index to select the row\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b2d39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Merging data frames\n",
    "\n",
    "You want to merge two data frames on a given column from each. Just like a join in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6141df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a data frame, mapping story numbers to titles\n",
    "data = {\n",
    "        'storyid': [1,2,3], \n",
    "        'title': ['lions', 'tigers', 'bears']\n",
    "}\n",
    "stories_df = pd.DataFrame.from_dict(data)\n",
    "stories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2cb39e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make another data frame with the data and story numbers, including ratings\n",
    "data = {\n",
    "        'subject': [1,1,1,2,2,2],\n",
    "        'storyid': [1,2,3,2,3,1],\n",
    "        'rating': [6.7,4.5,3.7,3.3,4.1,5.2]\n",
    "}\n",
    "rating_df = pd.DataFrame.from_dict(data)\n",
    "rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95db84f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Merge the two data frames\n",
    "# how='left' is the default (left join), so it is not required to provide it, but for clarity we do\n",
    "# Here we provide it in order to make the code more readable\n",
    "stories_df.merge(rating_df, on=['storyid'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a87436",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Merging data frames on more than one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make up more data: create a dataframe with animals and a dataframe with observations\n",
    "data = {\n",
    "        'size': ['small', 'big', 'small', 'big'],\n",
    "        'type': ['cat', 'cat', 'dog', 'dog'],\n",
    "        'name': ['lynx', 'tiger', 'chihuahua', 'great dane']\n",
    "}\n",
    "animals_df = pd.DataFrame.from_dict(data)\n",
    "print(animals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ccfc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = {'number':[1,2,3,4], 'size': ['big', 'small', 'small', 'big'], 'type': ['cat', 'dog', 'dog', 'dog']}\n",
    "observations_df = pd.DataFrame.from_dict(data)\n",
    "print(f\"\\n{observations_df}\" )\n",
    "\n",
    "# Merge the dataframes\n",
    "observations_df.merge(animals_df, on=['size', 'type'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52010012",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding and removing duplicate records in a serie\n",
    "\n",
    "You want to find and/or remove duplicate entries from a serie or data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a6bd5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Python has various data structures you can use to store data\n",
    "# We already used the dictionary and the pandas dataframe\n",
    "# You can also store data in lists, arrays and series\n",
    "\n",
    "# Python list\n",
    "my_list = [1, 2, 3, 4, 5]\n",
    "print(my_list)\n",
    "\n",
    "# Numpy array\n",
    "my_array = np.array([1, 2, 3, 4, 5])\n",
    "print(f\"\\n{my_array}\")\n",
    "\n",
    "# Pandas series\n",
    "my_series = pd.Series(my_array)\n",
    "print(f\"\\n{my_series}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4751d5e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# import the required packages we will be using\n",
    "from numpy.random import seed\n",
    "from numpy.random import normal\n",
    "\n",
    "# Generate a series \n",
    "seed(3)\n",
    "x = pd.Series(normal(loc=10, scale=5, size=20).round()).astype(int)\n",
    "print(*x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda637d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# For each element: is this one a duplicate (first instance of a particular value not counted)\n",
    "x_duplicates = x.duplicated()\n",
    "print(*x_duplicates)\n",
    "\n",
    "# The values of the duplicated entries\n",
    "# note that '10' appears in the original vector four times, and so it has three entries here\n",
    "x_duplicates_values = x[x.duplicated()]\n",
    "print(*x_duplicates_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b3aef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create a list with duplicates\n",
    "x_list = [1, 2, 3, 3, 4, 4, 5]\n",
    "\n",
    "# a set is another python data structure: it can only contains unique items\n",
    "# and therefore we can use it to remove duplicates and convert the set back to a list again\n",
    "list(set(x_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9ea36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finding and removing duplicate records in data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9964ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample data frame:\n",
    "data = {\n",
    "        'label': ['A', 'B', 'C', 'B', 'B', 'A', 'A', 'A'],\n",
    "        'type': [4,3,6,3,1,2,4,4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42064928",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data)\n",
    "print(df)\n",
    "\n",
    "# Is each row a repeat?\n",
    "x_duplicates = df.duplicated()\n",
    "x_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c3998",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_duplicates_values = df[df.duplicated()]\n",
    "df_duplicates_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e9f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show unique repeat entries \n",
    "df_duplicates_values_unique = df_duplicates_values.drop_duplicates()\n",
    "df_duplicates_values_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7e501",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Original data with repeats removed. These do the same:\n",
    "df_unique1 = df.drop_duplicates()\n",
    "df_unique1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4581cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique2 = df[~df.duplicated()]\n",
    "df_unique2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d803df8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducing Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c671e55",
   "metadata": {},
   "source": [
    "At any Python Q&A site, you’ll frequently see an exchange like this one:\n",
    "\n",
    "Q: How can I use a loop to [...insert task here...] ?\n",
    "A: Don't. Use one of the apply functions or the Python list comprehension\n",
    "\n",
    "So, what are these wondrous apply functions and list comprehensions and how do they work? I think the best way to figure out anything in Python is to learn by experimentation, using embarrassingly trivial data and functions.\n",
    "\n",
    "Let’s examine some of those.\n",
    "\n",
    "TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c88cc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### apply_along_axis\n",
    "\n",
    "Description : Returns a series or array or list of values obtained by applying a function to margins of an array or matrix.\n",
    "\n",
    "OK. We know about series/arrays and functions, but what are these “margins”?\n",
    "\n",
    "Simple: either the rows (0), the columns (1) or both. By both, we mean apply the function to each individual value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad55c31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# An example:\n",
    "\n",
    "# create a matrix of 10 rows x 2 columns\n",
    "# Python doesn't have a built-in type for matrices.\n",
    "# However, we can treat a list of a list as a matrix\n",
    "a = np.array([range(1,11)])\n",
    "b = np.array([range(11,21)])\n",
    "m = np.vstack((a,b))\n",
    "print(m.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cffd8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# mean of the rows\n",
    "mean_rows = np.apply_along_axis(np.mean, 0, m)\n",
    "print(*mean_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31909659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of the columns\n",
    "mean_cols = np.apply_along_axis(np.mean, 1, m)\n",
    "print(*mean_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38236ef8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Divide all values by 2\n",
    "# notice that here the axis argument is simply ignored but a value has to be provided\n",
    "div_2 = np.apply_along_axis(lambda x: x/2, 1, m)\n",
    "print(div_2.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085bc9cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Groupby\n",
    "\n",
    "To illustrate, we can load up the classic iris dataset, which contains a bunch of flower measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8225d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we saw before, the iris data is available in the seaborn package:\n",
    "import seaborn as sns\n",
    "# The following script loads the iris data set into a data frame\n",
    "df_iris_seaborn = sns.load_dataset('iris')\n",
    "df_iris_seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b5686",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_iris_seaborn.groupby('species').agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bfe59b",
   "metadata": {},
   "source": [
    "Essentially, groupby provides a way to split your data by factors and do calculations on each subset. It returns an object of class “groupby” and there are many more complex ways to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cd325",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### map\n",
    "\n",
    "Description : map returns a map object of the same length as X, each element of which is the result of applying FUN to the corresponding element of X\n",
    "\n",
    "Maps are lazily evaluated, meaning the values are only computed on-demand. \n",
    "\n",
    "A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "l = [list(range(1,11)), list(range(11,21))]\n",
    "mean_map=map(statistics.mean, l)\n",
    "print(list(mean_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af95d94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# the sum of the values in each element\n",
    "sum_map=map(sum, l)\n",
    "print(list(sum_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice: alternatively, you can also use list comprehension:\n",
    "sum_list=[sum(i) for i in l]\n",
    "print(sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbacfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate multiple values\n",
    "mult_funcs = [min, statistics.median, max]\n",
    "\n",
    "mean_sum_list = [[x(i) for x in mult_funcs] for i in l]\n",
    "print(mean_sum_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bfd4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Replicate with NumPy\n",
    "NumPy is a package for working with numerical data and used for data science and scientific computing.\n",
    "\n",
    "An example: let’s simulate 5 normal distributions, each with 5 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8fb56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "np.random.normal(loc=0.48, scale=0.05, size=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74c1dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Replicate a function on elements with map\n",
    "With map we can apply a function to multiple list arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee721ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_to_four = range(1, 5)\n",
    "four_to_one = range(4, 0, -1)\n",
    "    \n",
    "rep = lambda value, times: [value]*times\n",
    "list(map(rep, one_to_four, four_to_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5555c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## dfply\n",
    "\n",
    "The dfply package makes it possible to do R's dplyr-style data manipulation with pipes in Python on pandas DataFrames.\n",
    "\n",
    "This is an alternative to pandas-ply and dplython, which both engineer dplyr syntax and functionality in Python. There are probably more packages that attempt to enable dplyr-style dataframe manipulation in Python, but those are the two I am aware of.\n",
    "\n",
    "dfply uses a decorator-based architecture for the piping functionality and to \"categorize\" the types of data manipulation functions. The goal of this architecture is to make dfply concise and easily extensible, simply by chaining together different decorators that each have a distinct effect on the wrapped function.\n",
    "\n",
    "dfply is intended to mimic the functionality of dplyr. The syntax is the same for the most part, but will vary in some cases as Python is a considerably different programming language than R.\n",
    "\n",
    "from <a href=\"https://github.com/kieferk/dfply\"> kieferk/dfply </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc60b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## dfply\n",
    "\n",
    "When working with data you must:\n",
    "\n",
    "* Figure out what you want to do.\n",
    "* Precisely describe what you want in the form of a computer program.\n",
    "* Execute the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f745f94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The dfply package makes each of these steps as fast and easy as possible by:\n",
    "\n",
    "* Elucidating the most common data manipulation operations, so that your options are helpfully constrained when thinking about how to tackle a problem.\n",
    "* Providing simple functions that correspond to the most common data manipulation verbs, so that you can easily translate your thoughts into code.\n",
    "* Using efficient data storage backends, so that you spend as little time waiting for the computer as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb006120",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install dfply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d047ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data: nycflights13\n",
    "\n",
    "To explore the basic data manipulation verbs of dfply, we’ll start with the built-in nycflights13 data frame. This dataset contains all 336776 flights that departed from New York City in 2013. The data comes from the <a href=\"https://www.bts.gov/\">US Bureau of Transporation Statistics</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30feb08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dfply import *\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flight_data = pd.read_csv('./datasets/flights.csv')\n",
    "flight_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff13242",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "flight_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a580ffd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single table verbs\n",
    "\n",
    "Dfly aims to provide a function for each basic verb of data manipulating:\n",
    "\n",
    "* filter_by() (and row_slice())\n",
    "* arrange()\n",
    "* select() (and rename())\n",
    "* distinct()\n",
    "* mutate() (and transmute())\n",
    "* summarize()\n",
    "* sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f8f966",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# dfply works directly on pandas DataFrames, chaining operations on the data with the >> operator\n",
    "\n",
    "flight_data >> head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ff03e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The DataFrame as it is passed through the piping operations is represented by the symbol X\n",
    "flight_data >> select(X.dep_time, X.origin) >> head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f908a24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Arrange rows with arrange()\n",
    "\n",
    "arrange() works similarly to filter() except that instead of filtering or selecting rows, it reorders them. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data >> arrange(X.year, X.month, X.day) >> head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5d6fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use desc() to order a column in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data >> arrange(X.arr_delay, ascending=False) >> head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e1e53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filter rows with filter_by()\n",
    "\n",
    "filter_by() allows you to select a subset of the rows of a data frame. The first and subsequent arguments are filtering expressions evaluated in the context of that data frame.\n",
    "\n",
    "For example, we can select all flights on February 3rd with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54563c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data >> filter_by(X.month==2, X.day==3) >> head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cbf13a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More filter() and slice()\n",
    "\n",
    "This is equivalent as it can be done using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab55d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# & = AND | = OR\n",
    "# select first of januari\n",
    "flight_data[(flight_data[\"month\"] == 1) & (flight_data[\"day\"] == 1)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a553d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To select rows by position, use row_slice(). You can pass single integer indices or a list of indices to select rows as with. This is going to be the same as using pandas' .iloc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33585aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data >> row_slice([0,7,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b6011",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Select columns with select()\n",
    "\n",
    "Often you work with large datasets with many columns where only a few are actually of interest to you. select() allows you to rapidly zoom in on a useful subset using operations that usually only work on numeric variable positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d940778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns by name\n",
    "flight_data >> select(X.year, X.month, X.day) >> head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be56895",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The select function accept string labels, integer positions, and/or symbolically represented column names (X.column)\n",
    "flight_data >> select(X.year, ['month', 'day'],6) >> head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284dbfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extract distinct (unique) rows\n",
    "\n",
    "A common use of select() is to find out which values a set of variables takes. This is particularly useful in conjunction with the distinct() verb which only returns the unique values in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ee81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data >> select(X.tailnum) >> distinct(X.tailnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ec6db3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grouped operations\n",
    "\n",
    "These verbs are useful, but they become really powerful when you combine them with the idea of “group by”, repeating the operation individually on groups of observations within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2ccdf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The verbs are affected by grouping as follows:\n",
    "\n",
    "* grouped select() is the same as ungrouped select(), except that grouping variables are always retained.\n",
    "* grouped arrange() orders first by grouping variables\n",
    "* mutate() and filter() are most useful in conjunction with window functions (like rank(), or min(x) == x), and are described in detail in vignette(“window-function”).\n",
    "* sample() samples the specified number/fraction of rows in each group.\n",
    "* row_slice() extracts rows within each group.\n",
    "* summarize() and summarize_each() are easy to understand and very useful, and is described in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b20d71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example group by\n",
    "\n",
    "In the following example, we split the complete dataset into individual planes and then summarise each plane by counting the number of flights (count = n()) and computing the average distance (distance_mean=X.distance.mean()) and delay (arr_delay_mean=X.arr_delay.mean())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43881ad0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "delay_df = (flight_data >> \n",
    " group_by(X.tailnum) >> \n",
    " summarize(tailnum_count=n(X.tailnum), distance_mean=X.distance.mean(), arr_delay_mean=X.arr_delay.mean()) >>\n",
    " filter_by(X.tailnum_count >20, X.distance_mean<2000)\n",
    ")\n",
    "delay_df >> head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66636df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plot delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d96373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the seaborn package for making a visualisation\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea1fcf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot miles per gallon against horsepower with other semantics\n",
    "sns.relplot(x=\"distance_mean\", y=\"arr_delay_mean\", size=\"tailnum_count\",\n",
    "            sizes=(40, 400), alpha=.5, palette=\"muted\",\n",
    "            height=6, data=delay_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abf5dd7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Add new columns with mutate()\n",
    "\n",
    "As well as selecting from the set of existing columns, it’s often useful to add new columns that are functions of existing columns. This is the job of mutate():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad61ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "(flight_data >>\n",
    " mutate(\n",
    "  gain = X.arr_delay - X.dep_delay,\n",
    "  speed = X.distance / X.air_time * 60) >>\n",
    " head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7c4ef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: In Python the new variables created with mutate may not be guaranteed to be created in the same order that they are input into the function call, though this may have been changed in Python 3.x. For example, the following will generate an error:\n",
    "\n",
    "    NameError: name 'gain' is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76dc9bc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(flight_data >>\n",
    " mutate(\n",
    "  gain = X.arr_delay - X.dep_delay,\n",
    "  gain_per_hour = gain / (X.air_time / 60)\n",
    " ) >>\n",
    "head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417e3a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combine Datasets\n",
    "\n",
    "* Mutating joins\n",
    "* Filtering joins\n",
    "* Set operations\n",
    "* Binding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372982a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Randomly sample rows with sample()\n",
    "\n",
    "You can use sample() to take a random sample of rows, either a fixed number for sample(n=3, replace=True) or a fixed fraction for sample(frac=0.0002, replace=False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ed367",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data >> sample(n=3, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30f37f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "flight_data >> sample(frac=0.00002, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2061c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Commonalities\n",
    "\n",
    "You may have noticed that all these functions are very similar:\n",
    "\n",
    "* A data frame is piped to the function as input\n",
    "* The arguments describe what to do with it, and you can refer to columns in the data frame directly.\n",
    "* The result is a new data frame\n",
    "\n",
    "Together these properties make it easy to chain together multiple simple steps to achieve a complex result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607ebc2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Databases and Python\n",
    "\n",
    "As well as working with local in-memory data like data frames and data tables, Pandas also works with remote on-disk data stored in databases. Generally, if your data fits in memory there is no advantage to putting it in a database: it will only be slower and more hassle.\n",
    "\n",
    "The reason you’d want to use Pandas with a database is because either your data is already in a database (and you don’t want to work with static csv files that someone else has dumped out for you), or you have so much data that it does not fit in memory and you have to use a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952463b3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Load data in a (SQLite) database\n",
    "\n",
    "Using a SQLite database in Python is really easy: just give it a path and the ok to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae0954",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# we need the sqlite3 package for this\n",
    "import sqlite3\n",
    "\n",
    "# create a connection to the database\n",
    "conn = sqlite3.connect('file:cachedb?mode=memory&cache=shared')\n",
    "cur  = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe89654",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The main new concept here is the connect() method, which is a collection of tables. Use sqlite3.connect() to connect to the different sqlite3 databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278e009",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Loading data\n",
    "\n",
    "This is a quick and dirty way of getting data into a database. The following will create a table called \"flight_data\" and store the data of the flight_data dataframe in it. A file will be cretaed in the current working folder called \"file:cachedb?mode=memory&cache=shared\" where the data of the database will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee8add",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# create MultiIndexes on some columns in the dataframe:\n",
    "flight_data_multiIndexes = ['year', 'month', \"day\", \"carrier\", \"tailnum\"]\n",
    "flight_data.set_index(flight_data_multiIndexes, inplace=True)\n",
    "\n",
    "# load the data in the new SQLite database\n",
    "flight_data.to_sql(\"flight_data\", conn, index=True, if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ca88b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As you can see, the to_sql() operation has an additional argument that allows you to indicate to used the dataframe indexes as indexes for the table as well. Here we set up indexes that will allow us to quickly process the data by day, by carrier and by plane.\n",
    "\n",
    "Did you know that <a href=\"https://sqlite.org/index.html\">SQLite</a> the most widely used SQL databased engine in the world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c6eb7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Retrieving data from the database\n",
    "\n",
    "For this particular dataset, there’s a built src that will cache flights in a standard location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6dc53a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# load data from your connection\n",
    "flight_data_df = pd.read_sql_query(\"select * from flight_data\", conn, index_col=flight_data_multiIndexes)\n",
    "flight_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b5f40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other useful packages for datamunging in Python\n",
    "\n",
    "* If you want to connect to a database other than SLQLite, use <a href=\"https://www.sqlalchemy.org/\">SQLAlchemy</a>\n",
    "\n",
    "* Alternatives for dfply include <a href=\"https://github.com/coursera/pandas-ply\">pandas-ply</a> and <a href=\"https://github.com/dodger487/dplython\">Dplython</a>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "rise": {
   "autolaunch": false,
   "backimage": "./png/python-achtergrond-dikw-licht.png",
   "enable_chalkboard": false,
   "theme": "simpel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
