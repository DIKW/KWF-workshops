{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# P40 Topic Modelling\n",
    "\n",
    "Automatic topic detection in large corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics\n",
    "\n",
    "* Word frequencies\n",
    "* Topic models\n",
    "* (Sentiment analysis)\n",
    "\n",
    "\n",
    "<img align=\"middle\" src=\"./png/text_mining.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word frequencies\n",
    "\n",
    "What is the most simple analysis we can do on a document?\n",
    "\n",
    "Count the frequencies of all words used in the document!\n",
    "\n",
    "Let’s see were this takes us…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Documents as a Bag of words\n",
    "\n",
    "<img align=\"middle\" src=\"./png/bow.png\" width=\"300\"/>\n",
    "\n",
    "* First step is to transform text into a ‘Bag-Of-Words’\n",
    "* This is a matrix with all the unique words and their frequencies (how often they occur) per document\n",
    "* Each word is a feature in this matrix\n",
    "\n",
    "<img align=\"middle\" src=\"./png/bow_table.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term frequency and inverse document frequency\n",
    "\n",
    "One way to measure how important a word is in a document is by counting the *term frequency* for this word.\n",
    "\n",
    "This will result in a lot of words that occur very frequently but we know are not important like “the”, “is”, “of”.\n",
    "\n",
    "So another way to assess the importance of a word in a corpus of documents is to look at the *inverse document frequency* of this word. Doing so will decrease weight for common words and increase weight for less common words.\n",
    "\n",
    "So for inverse document frequency we define:\n",
    "\n",
    "$$idf(term) = ln(\\frac{n_{documents}}{n_{documents containing term}})$$\n",
    "\n",
    "And multiplying gives:\n",
    "\n",
    "$$tfidf = tf∗idf$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Top 2000 dataset\n",
    "\n",
    "<img align=\"middle\" src=\"./png/top2000.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "In this class we will use the Top 2000 dataset.\n",
    "\n",
    "It can be found in the zipfile data_for_windows.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wordcloud\n",
    "\n",
    "Which song is visualised in this image?\n",
    "\n",
    "<img align=\"middle\" src=\"./png/song.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Make your own wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import required modules\n",
    "\n",
    "    ! pip install wordcloud\n",
    "    ! pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to download some data from nltk\n",
    "import nltk\n",
    "# a gui screen will open to download relevant stuff\n",
    "# \n",
    "#nltk.download('punkt') # 'punkt' 'stopwords'\n",
    "#nltk.download('stopwords') # 'punkt' 'stopwords' 'wordnet' 'omw-1.4'\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all libraries we need\n",
    "import wordcloud as wc\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import zipfile\n",
    "from nltk.corpus.reader import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extract and load Top2000 data\n",
    "\n",
    "Extract the data we will be using from the zip file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust to your likings\n",
    "path_to_zip_file = \"./data/top2000.zip\"\n",
    "directory_to_extract_to = \"./data/t2000\"\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# read one song\n",
    "data_path_file_name = directory_to_extract_to + \"/top2000/lyrics/The_Beatles_Can_t_Buy_Me_Love.txt\"\n",
    "with open(data_path_file_name) as f:\n",
    "    text_raw = f.read()\n",
    "print(text_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stopwords\n",
    "Stopwords are usually excluded, because they affect the result with less informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_stopwords = set(wc.STOPWORDS)\n",
    "print(wc_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud = wc.WordCloud(stopwords = wc_stopwords, \n",
    "                max_words = 20,\n",
    "                collocations = False,\n",
    "                max_font_size=80).generate(text_raw)\n",
    "plt.imshow(wordcloud, interpolation='bilinear') \n",
    "plt.axis(\"off\")\n",
    "plt.show()                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Data structures\n",
    "\n",
    "Structuring text data can be done in different ways. This is worth contrasting with the ways text is often stored in text mining approaches.\n",
    "\n",
    "* *String* : Text can, of course, be stored as strings.\n",
    "* *Corpus* : These types of objects typically contain raw strings annotated with additional metadata and details.\n",
    "* *Document-term matrix* : This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count or tf-idf.\n",
    "* *tidy text* : from the R language the concept of [tidy data principles](https://towardsdatascience.com/what-is-tidy-data-d58bb9ad2458) also holds for text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Corpus\n",
    "\n",
    "If we have to deal with a lot of documents we can create a structured object for it.\n",
    "\n",
    "We will be using the nltk corpus reader package: https://www.nltk.org/api/nltk.corpus.reader.html\n",
    "\n",
    "Structure the text documents in a corpus can be done like so:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = directory_to_extract_to + \"/top2000/lyrics/\"\n",
    "file_ext = \"txt\"\n",
    "file_ids = [f for f in listdir(corpus_root) if isfile(join(corpus_root, f)) and f.lower().endswith(file_ext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdir(corpus_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = PlaintextCorpusReader(corpus_root, file_ids)\n",
    "print(\"The number of documents:\", len(corpus.fileids()))\n",
    "print(\"The number of sentences =\", len(corpus.sents()))\n",
    "print(\"The number of words =\", len([word for sentence in corpus.sents() for word in sentence]))\n",
    "print(\"The number of characters =\", len([char for sentence in corpus.sents() for word in sentence for char in word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document-term matrix\n",
    "\n",
    "A document-term matrix contains terms with their frequencies of all documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vect = CountVectorizer(max_df=2)\n",
    "# term document matrix (more efficient for large corpora)\n",
    "term_document_matrix = count_vect.fit_transform([corpus.raw(i) for i in file_ids])\n",
    "df_dtm = pd.DataFrame(term_document_matrix.toarray(), columns=count_vect.get_feature_names_out())\n",
    "df_dtm['file_ids'] = file_ids\n",
    "df_dtm=df_dtm.set_index('file_ids')\n",
    "df_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Frequent terms\n",
    "\n",
    "Filter most frequent terms in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FreqDist requires a list of words as input\n",
    "# We will lowercase the text in each document in the corput, join it with the other documents into one long string\n",
    "# and finally split the string into words and store them in a list\n",
    "freq = nltk.FreqDist(' '.join([corpus.raw(i).lower() for i in file_ids]).split())\n",
    "top_words = freq.most_common(10)\n",
    "top_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of stopwords! What about ‘ain’ and ‘don’?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clean text\n",
    "\n",
    "Let’s clean the text from stopwords, whitespace, numbers and punctuation\n",
    "\n",
    "There is also a package named [textcleaner](https://pypi.org/project/textcleaner/) you can use \n",
    "\n",
    "Based on [this SO answer](https://stackoverflow.com/questions/54396405/how-can-i-preprocess-nlp-text-lowercase-remove-special-characters-remove-numb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes some time\n",
    "df = pd.DataFrame(columns=['Text'])\n",
    "df['text'] = [corpus.raw(i) for i in file_ids]\n",
    "df['file_ids'] = file_ids\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re # regular expressions\n",
    "# optional lemanize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# optional stemmer\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')          # sentence > sequence of words\n",
    "    tokens = tokenizer.tokenize(rem_num)         # remove words that only contain numbers\n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "\n",
    "df['clean_text'] = df['text'].map(lambda s:preprocess(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# check results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clean = df[['file_ids','clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "corpus_clean['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Find popular terms after cleaning\n",
    "\n",
    "Popular terms in the Top 2000, notice that we now supply a dataframe column as input for transformation to count_vect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(' '.join(corpus_clean['clean_text']).split())\n",
    "topWords = freq.most_common(20)\n",
    "topWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lines per song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are back to the raw corpus\n",
    "newline_count_file = [[corpus.raw(i).count('\\n'), i] for i in file_ids]\n",
    "newline_count_file_sorted = sorted(newline_count_file, key=lambda x: -x[0])[0:10]\n",
    "ys, xs = [*zip(*newline_count_file_sorted)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plt.barh(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Most used word in one song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(columns=['word', 'n', 'total'])\n",
    "for i in file_ids:\n",
    "    list_with_words = ' '.join([corpus.raw(i).lower()]).split()\n",
    "    freq = nltk.FreqDist(list_with_words)\n",
    "    df1.loc[i] = [freq.most_common(1)[0][0], freq.most_common(1)[0][1], len(list_with_words)]\n",
    "df1.sort_values(\"n\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# example songs\n",
    "song_list = [\"Pearl_Jam_Black.txt\", \"James_Brown_Sex_Machine.txt\",\n",
    "               \"The_Blues_Brothers_Everybody_Needs_Somebody_To_Love.txt\",\n",
    "               \"Justin_Timberlake_Cry_Me_A_River.txt\"]\n",
    "\n",
    "for song in song_list:\n",
    "    cnt = Counter()\n",
    "    total_words = len(corpus.raw(song).lower().split())\n",
    "    \n",
    "    for text in corpus.raw(song).lower().split():\n",
    "        cnt[text] += 1\n",
    "    # See most common ten words\n",
    "    cnt.most_common(10)\n",
    "\n",
    "    word_freq = pd.DataFrame(cnt.most_common(20), columns=['words', 'count'])\n",
    "    word_freq[\"total_words\"] = total_words\n",
    "    word_freq[\"n_total\"] = round(word_freq.apply(lambda row: row[\"count\"] / row.total_words, axis=1),2)\n",
    "    #word_freq.head()\n",
    "    fig, ax = plt.subplots(figsize=(12, 3))\n",
    "\n",
    "    # Plot horizontal bar graph\n",
    "    word_freq.sort_values(by='count').plot.bar(x='words',\n",
    "                        y='count',\n",
    "                        ax=ax,\n",
    "                        color=\"brown\")\n",
    "    ax.set_title(song)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "One way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This isn’t the only way to approach sentiment analysis, but it is an often-used approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install random-word\n",
    "# !pip install vadersentiment\n",
    "nltk.download(\"vader_lexicon\")\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "word_list = [\"yes\", \"no\", \"abort\", \"not yes\", \"upset\", \"happy\", \"angry\", \"holiday\"]\n",
    "for word in word_list:\n",
    "    \n",
    "    sentiments = [vader.polarity_scores(word)]\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    print(f\"Sentiment {sentiments}\")\n",
    "    sentiment_scores = SentimentIntensityAnalyzer().polarity_scores(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentiment lexicons\n",
    "\n",
    "There are general-purpose lexicons available for doing sentiment analysis with Python such as:\n",
    "\n",
    "* AFINN from Finn Årup Nielsen,\n",
    "* Vader, and\n",
    "* pre-trained AI sentiment models \n",
    "\n",
    "The Vader lexicon. VADER stands for Valence Aware Dictionary for sEntiment Reasoner) and has a model that can deal with problem text like “not great” (ie, negations) and is also sensitive to intensity of language or amplifiers (“very happy” vs “happy”).\n",
    "\n",
    "The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.  AFINN preprocesses text by removing the punctuation and converting all the words to lower-case.\n",
    "\n",
    "There are many pre-trained AI models models available which can be used to better suit your language on use case. These mostly require TensorFlow or PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explore the sentiment lexicons yourself\n",
    "\n",
    "Dictionary-based methods like the ones we are discussing find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install afinn\n",
    "from afinn import Afinn\n",
    "afinn = Afinn(language='en')\n",
    "# The score method returns the sum of word valence scores for a text string.\n",
    "afinn.score('I had a bad day.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentiment scores with inner join\n",
    "\n",
    "We use four songs to do the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(song_list)\n",
    "df_corps = df.set_index('file_ids', inplace=False)\n",
    "df_sentiment_selected = df_corps.loc[song_list]\n",
    "df_sentiment_selected\n",
    "sentiments = [vader.polarity_scores(document) for document in df_sentiment_selected['clean_text']]\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take sections of ten lines and calculate sentiment on each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install more_itertools\n",
    "from more_itertools import grouper\n",
    "\n",
    "def group_lines(iterable, n=10):\n",
    "    return [\"\\n\".join((line for line in lines if line))\n",
    "            for lines in grouper(n, iterable.split(\"\\n\"), fillvalue=\"\")]\n",
    "\n",
    "[document for document in group_lines(df_corps.loc['Pearl_Jam_Black.txt']['text'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sentiment scores\n",
    "\n",
    "Now we can plot these sentiment scores across the duration of the song.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, nrows=2, sharex=True, sharey=True, figsize=(12,8))\n",
    "\n",
    "song_index=0\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    song = song_list[song_index]\n",
    "    song_index = song_index + 1\n",
    "\n",
    "    split_lines = group_lines(df_corps.loc[song]['text'])\n",
    "\n",
    "    split_lines_sentiments = [vader.polarity_scores(document) for document in split_lines]\n",
    "    split_lines_sentiments_compound = [item[\"compound\"] for item in split_lines_sentiments]\n",
    "\n",
    "    ax.bar([*range(1, len(split_lines)+1, 1)], split_lines_sentiments_compound)\n",
    "    ax.set_title(song)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Homework\n",
    "\n",
    "Explore the impact of choosing a different lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic Modeling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import _stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features limits the number of features to use\n",
    "vect = CountVectorizer(max_features=1000,ngram_range=(1,1),stop_words=['english','dutch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a document term matrix\n",
    "dtm=vect.fit_transform(corpus_clean['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document term matrix\n",
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dtm.toarray(),columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Latent Dirichlet Allocation\n",
    "\n",
    "\n",
    "Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.\n",
    "\n",
    "The graphical model of LDA is a three-level generative model:\n",
    "\n",
    "<img align=\"middle\" src=\"./png/lda_model_graph.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "Note on notations presented in the graphical model above, which can be found in Hoffman et al. (2013):\n",
    "\n",
    "* The corpus is a collection of D documents.\n",
    "* A document is a sequence of N words.\n",
    "* There are K topics in the corpus.\n",
    "* The boxes represent repeated sampling.\n",
    "\n",
    "In the graphical model, each node is a random variable and has a role in the generative process. A shaded node indicates an observed variable and an unshaded node indicates a hidden (latent) variable. In this case, words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure.\n",
    "\n",
    "[more on scikitlearn](https://scikit-learn.org/stable/modules/decomposition.html#latent-dirichlet-allocation-lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# how many topics do we want to find\n",
    "lda=LatentDirichletAllocation(n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "lda.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualization of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zit=pyLDAvis.sklearn.prepare(lda,dtm,vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(zit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# function to get relevant words that define the topics\n",
    "def get_model_topics(model, vectorizer, topics, n_top_words=5):\n",
    "    word_dict = {}\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        word_dict[topics[topic_idx]] = top_features\n",
    "\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What do the topics mean?\n",
    "\n",
    "So now we have found a latent clustering of relevant words into topics.\n",
    "\n",
    "And we can use this to predict for a new doucment which topics are talked about in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do the topics mean?\n",
    "topics = ['T1','T2','T3','T4','T5','T6','T7','T8','T9','T10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most relevant wordes describe the topic\n",
    "get_model_topics(lda,vect,topics,n_top_words=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Use the lda model to predict what topic is discussed in a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference(model, vectorizer, topics, text, threshold):\n",
    "    v_text = vectorizer.transform([text])\n",
    "    score = model.transform(v_text)\n",
    "\n",
    "    labels = set()\n",
    "    for i in range(len(score[0])):\n",
    "        if score[0][i] > threshold:\n",
    "            labels.add(topics[i])\n",
    "\n",
    "    if not labels:\n",
    "        return 'None', -1, set()\n",
    "\n",
    "    return topics[np.argmax(score)], score, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a Dutch song\n",
    "text = corpus_clean.iloc[3]['clean_text']\n",
    "# there is topic that is about 'Dutch' songs ...\n",
    "(topic, scores, topic_labels) = get_inference(lda, vect, topics, text, threshold=0.0)\n",
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# get topic scores for each document\n",
    "doc_topic_dist_unnormalized = np.matrix(lda.transform(dtm))\n",
    "\n",
    "# normalize the distribution (only needed if you want to work with the probabilities)\n",
    "doc_topic_dist = doc_topic_dist_unnormalized/doc_topic_dist_unnormalized.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the topic with highest probability\n",
    "doc_topic_dist.argmax(axis=1)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise : filter Dutch songs only and do topic model on the Dutch songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aantal Nederlandse liedjes\n",
    "nl_topic = 5  # this can be different every time as we cannot predict the order in which the topics arefound\n",
    "nl_filter = doc_topic_dist.argmax(axis=1)==nl_topic\n",
    "corpus_clean_nl = corpus_clean[nl_filter.A1]\n",
    "corpus_clean_nl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_features limits the number of features to use\n",
    "vect2 = CountVectorizer(max_features=500,ngram_range=(1,1),stop_words=['dutch'])\n",
    "# build a document term matrix\n",
    "dtm_nl = vect2.fit_transform(corpus_clean_nl['clean_text'])\n",
    "# how many topics do we want to find\n",
    "lda_nl = LatentDirichletAllocation(n_components=10)\n",
    "# fit the model\n",
    "lda_nl.fit_transform(dtm_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most relevant wordes describe the topic\n",
    "get_model_topics(lda_nl,vect2,topics,n_top_words=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zit2 = pyLDAvis.sklearn.prepare(lda_nl,dtm_nl,vect2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(zit2)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cdsp] *",
   "language": "python",
   "name": "conda-env-cdsp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "rise": {
   "autolaunch": false,
   "backimage": "./png/python-achtergrond-dikw-licht.png",
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "simpel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
